[
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Data from the ds labs package will be utilized for this practice. The renv package is a great resource for keeping track of all the packages used within a project. I chose to install it here, so I can use it on class exercises and the final project. I loaded tidyverse to practice processing data and ggplot2 to visualize the processed data later in this exercise. I loaded naniar to determine missingness of the data.\n\nlibrary(dslabs)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(naniar)"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#exploring-gapminder",
    "href": "coding-exercise/coding-exercise.html#exploring-gapminder",
    "title": "R Coding Exercise",
    "section": "Exploring Gapminder",
    "text": "Exploring Gapminder\nI will explore the gapminder dataset included in this pacakge with the help(), str(), summary(), and class() functions.\n\nhelp(gapminder)\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\nclass(gapminder)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-data",
    "href": "coding-exercise/coding-exercise.html#processing-data",
    "title": "R Coding Exercise",
    "section": "Processing Data",
    "text": "Processing Data\nTo create a dataset with only African countries, I need to view the dataset using head() to determine a variable name that I can use to filter the dataset. Once I found “continent”, I filtered for Africa. The str() function shows me there are 2,907 observations of 9 variables, which means 2,907 African countries are included in the gapminder dataset. The summary() function allowed me to see what Dr. Handle meant by R keeping all the continent categories but plugging in a zero as a placeholder.\n\nhead(gapminder) ##I see a variable name called continent that I will use to filter the dataset\n\n              country year infant_mortality life_expectancy fertility\n1             Albania 1960           115.40           62.87      6.19\n2             Algeria 1960           148.20           47.50      7.65\n3              Angola 1960           208.00           35.98      7.32\n4 Antigua and Barbuda 1960               NA           62.97      4.43\n5           Argentina 1960            59.87           65.39      3.11\n6             Armenia 1960               NA           66.86      4.55\n  population          gdp continent          region\n1    1636054           NA    Europe Southern Europe\n2   11124892  13828152297    Africa Northern Africa\n3    5270844           NA    Africa   Middle Africa\n4      54681           NA  Americas       Caribbean\n5   20619075 108322326649  Americas   South America\n6    1867396           NA      Asia    Western Asia\n\nafricadata &lt;- gapminder %&gt;% \n                filter(continent == \"Africa\")  ##I created a new dataset that only includes African countries\nstr(africadata) \n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\nTo create two new objects from the ‘africadata’ object with only two columns each, I used the select() function to choose the columns I wanted to keep. The first object, ‘IMLE’, contains the infant mortality rates and life expectancies for each African country. I used the str() function to see there are 2907 observations of 2 variables, and the summary() function provides 5 data points for each numeric variable. The second object, “PLE’, contains the population and life expectancies for each African county. Similarly, I used str() to confirm there are 2907 observations of 2 variables, and the summary() function to learn about the range of each variable.\n\nIMLE &lt;- africadata %&gt;%  ##store a new object using the arrow notation\n        select(\"infant_mortality\", \"life_expectancy\") ##pipe in the filtered dataset and select specific columns \nstr(IMLE) ##confirm there are only 2 variables \n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(IMLE) ##observe the range of each numerical variable\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\nPLE &lt;- africadata %&gt;%  ##store a new object using the arrow notation\n          select(\"population\", \"life_expectancy\") ##pipe in the filtered dataset and select specific columns \nstr(PLE) ##confirm there are only 2 variables \n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(PLE) ##observe the range of each numerical variable\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#plotting",
    "href": "coding-exercise/coding-exercise.html#plotting",
    "title": "R Coding Exercise",
    "section": "Plotting",
    "text": "Plotting\nTo visualize the relationship between the two variables included in each of these objects, I will plot them using ggplot(). Using the IMLE object, I plotted infant mortality on the x axis and life expenctancy on the y axis using the aesthetics filter with ggplot. I added geom-point() to plot the data as points. I also chose to include a title, which I centered on the graph, to make the contents of the plot even clearer. This plot shows the inverse relationship between life expectancy and infant mortality. The graph shows a strong inverse relationship with only 3 major outliers. The majority of observations fall between an infant mortality rate of 30 to 150.\n\nIMLE %&gt;% \n  ggplot(aes(x=infant_mortality, y=life_expectancy)) + ##use aesthetic layer to set the axis variables\n  geom_point() + ##create a scatter plot\n  labs(title = \"Life Expectancy as a Function of Infant Mortality\") + ##label the graph with a title\n  theme(plot.title = element_text(hjust = 0.5)) ##center the title on the graph\n\nWarning: Removed 226 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\n\nI followed a similar process to plot life expectancy based on population using the PLE object. I used ggplot() again along with geom_point() to plot the data points. I used the same technique to create a title centered on the graph. I also added the scale_x_log10() function to convert the numeric population variable to the logarithmic scale, which helps to create a cleaner visualization. The graph shows the most observations around a population of 1 million with a life expectancy ranging from 40 to 60. There is one major outlier on the graph, but most observations are clustered to create easily definable streaks. The streaks of data in both of these graphs correspond to observations in each African country over the years included in the dataset, which range from 1960 to 2016. As time passes, life expectancy increases which causes the population size to increase. Since infant mortality has decreased over this time period, life expectancy is increasing. These trends show up as streaks in the data because each country’s observations are grouped together over time.\n\nPLE %&gt;% \n  ggplot(aes(x=population, y=life_expectancy)) + ##use aesthetic layer to set the axis variables\n  geom_point() + ##create a scatter plot\n  labs(title = \"Life Expectancy as a Function of Population\") + ##label the graph with a title\n  theme(plot.title = element_text(hjust = 0.5)) + ##center the title\n  scale_x_log10() ##change the x-axis to the log scale\n\nWarning: Removed 51 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-data-processing",
    "href": "coding-exercise/coding-exercise.html#more-data-processing",
    "title": "R Coding Exercise",
    "section": "More data processing",
    "text": "More data processing\nFinding out which observations have missing data is an important part of data analysis. I used gg_miss_var() to visualize which variables were missing information. While GDP accounts for the most missing observations, infant_mortality is the variable with the second most missing values. To determine which years have missing infant_mortality data, I used AI to learn how to use dplyr functions to filter for the specific missing data I am looking for. I filtered for observations where infant mortality rate was missing, and I selected the year column since that is the variable I am interested in. After printing the data, I realized it showed all 226 observations by year, so I used the unique() function to more efficiently determine which years had at least 1 missing infant mortality rate observation. Infant mortality rate is missing from 1960 to 1981 and again in 2016.\n\ngg_miss_var(africadata) ##explore missingness of the data\n\n\n\n\n\n\n\nmissing_years &lt;- africadata %&gt;%\n                  filter(is.na(infant_mortality)) %&gt;% ##find observations of infant mortality that are missing \n                  select(year) ##keep only the year column of the observations with missing data\nprint(missing_years) ##view the years with missing data\n\n    year\n1   1960\n2   1960\n3   1960\n4   1960\n5   1960\n6   1960\n7   1960\n8   1960\n9   1960\n10  1960\n11  1961\n12  1961\n13  1961\n14  1961\n15  1961\n16  1961\n17  1961\n18  1961\n19  1961\n20  1961\n21  1961\n22  1961\n23  1961\n24  1961\n25  1961\n26  1961\n27  1961\n28  1962\n29  1962\n30  1962\n31  1962\n32  1962\n33  1962\n34  1962\n35  1962\n36  1962\n37  1962\n38  1962\n39  1962\n40  1962\n41  1962\n42  1962\n43  1962\n44  1963\n45  1963\n46  1963\n47  1963\n48  1963\n49  1963\n50  1963\n51  1963\n52  1963\n53  1963\n54  1963\n55  1963\n56  1963\n57  1963\n58  1963\n59  1963\n60  1964\n61  1964\n62  1964\n63  1964\n64  1964\n65  1964\n66  1964\n67  1964\n68  1964\n69  1964\n70  1964\n71  1964\n72  1964\n73  1964\n74  1964\n75  1965\n76  1965\n77  1965\n78  1965\n79  1965\n80  1965\n81  1965\n82  1965\n83  1965\n84  1965\n85  1965\n86  1965\n87  1965\n88  1965\n89  1966\n90  1966\n91  1966\n92  1966\n93  1966\n94  1966\n95  1966\n96  1966\n97  1966\n98  1966\n99  1966\n100 1966\n101 1966\n102 1967\n103 1967\n104 1967\n105 1967\n106 1967\n107 1967\n108 1967\n109 1967\n110 1967\n111 1967\n112 1967\n113 1968\n114 1968\n115 1968\n116 1968\n117 1968\n118 1968\n119 1968\n120 1968\n121 1968\n122 1968\n123 1968\n124 1969\n125 1969\n126 1969\n127 1969\n128 1969\n129 1969\n130 1969\n131 1970\n132 1970\n133 1970\n134 1970\n135 1970\n136 1971\n137 1971\n138 1971\n139 1971\n140 1971\n141 1971\n142 1972\n143 1972\n144 1972\n145 1972\n146 1972\n147 1972\n148 1973\n149 1973\n150 1973\n151 1973\n152 1973\n153 1973\n154 1974\n155 1974\n156 1974\n157 1974\n158 1974\n159 1975\n160 1975\n161 1975\n162 1975\n163 1975\n164 1976\n165 1976\n166 1976\n167 1977\n168 1977\n169 1977\n170 1978\n171 1978\n172 1979\n173 1979\n174 1980\n175 1981\n176 2016\n177 2016\n178 2016\n179 2016\n180 2016\n181 2016\n182 2016\n183 2016\n184 2016\n185 2016\n186 2016\n187 2016\n188 2016\n189 2016\n190 2016\n191 2016\n192 2016\n193 2016\n194 2016\n195 2016\n196 2016\n197 2016\n198 2016\n199 2016\n200 2016\n201 2016\n202 2016\n203 2016\n204 2016\n205 2016\n206 2016\n207 2016\n208 2016\n209 2016\n210 2016\n211 2016\n212 2016\n213 2016\n214 2016\n215 2016\n216 2016\n217 2016\n218 2016\n219 2016\n220 2016\n221 2016\n222 2016\n223 2016\n224 2016\n225 2016\n226 2016\n\nunique(missing_years) ##organize the years with missing data\n\n    year\n1   1960\n11  1961\n28  1962\n44  1963\n60  1964\n75  1965\n89  1966\n102 1967\n113 1968\n124 1969\n131 1970\n136 1971\n142 1972\n148 1973\n154 1974\n159 1975\n164 1976\n167 1977\n170 1978\n172 1979\n174 1980\n175 1981\n176 2016\n\n\nAfter choosing 2000 for the year to study infant mortality rate based on missingness in the data, I filtered for observations made during 2000 only. I used the dim() function to confirm that I have 51 observations of 9 variables. The str() function provides the variable type for each variable, and the summary() function provides more details for each variable.\n\nyear2000 &lt;- africadata %&gt;% \n              filter(year == \"2000\") ##create a new object with only observations from the year 2000\ndim(year2000) ##confirm dimensions of year2000\n\n[1] 51  9\n\nstr(year2000) ##observe structure of year2000\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(year2000) ##view summaries of variables included in year 2000\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-plotting",
    "href": "coding-exercise/coding-exercise.html#more-plotting",
    "title": "R Coding Exercise",
    "section": "More plotting",
    "text": "More plotting\nI made the same plots as above, excpet there is only data from the year 2000 this time. I made the same design choices to create a clear plot using labs() and theme() to visualize Infant Morality and Life Expectancy in 2000 and Population and Life Expectancy in 2000. Infant mortality and life expectancy still show an inverse relationship, but the relationship between population and life expectancy is no longer visible.\n\nyear2000 %&gt;% \n  ggplot(aes(x=infant_mortality, y=life_expectancy)) + ##use aesthetic layer to set the axis variables\n  geom_point() + ##create a scatter plot\n  labs(title = \"Infant Mortality and Life Expectancy in 2000\") + ##label the graph with a title\n  theme(plot.title = element_text(hjust = 0.5)) ##center the title\n\n\n\n\n\n\n\nyear2000 %&gt;% \n  ggplot(aes(x=population, y=life_expectancy)) + ##use aesthetic layer to set the axis variables\n  geom_point() + ##create a scatter plot\n  labs(title = \"Infant Mortality and Life Expectancy in 2000\") + ##label the graph with a title\n  theme(plot.title = element_text(hjust = 0.5)) + ##center the title\n  scale_x_log10() ##convert the x axis to log sccale"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#simple-model-fits",
    "href": "coding-exercise/coding-exercise.html#simple-model-fits",
    "title": "R Coding Exercise",
    "section": "Simple model fits",
    "text": "Simple model fits\nTo explore the relationship between population and life expectancy in 2000, I will use the lm() function with the year2000 object to create a simple model comparing life expectancy to infant mortality in fit 1 and to population in fit 2. Based on the summary() function output for fit1, we can conclude that infant mortality rate has a statistically significant effect on life expectancy because the p-value for the coefficient estimate is 2.83e-08, which is much smaller than 0.05. For every 1 unit increase in infant mortality rate, life expectancy decreases by 0.18916 years. Based on the summary() function output for fit2, we can conclude that there is not a statistically significant relationship between life expectancy and population because the p-value for the coefficient estimate is 0.616, which is much greater than 0.05.\n\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = year2000) ##create a linear model with life expectancy as outcome and infant mortality as predictor and save it to an object called fit1\nsummary(fit1) ##generate information about linear regression equation with p-values\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = year2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nfit2 &lt;- lm(life_expectancy ~ population, data = year2000) ##create a linear model with life expectancy as outcome and population as predictor and save it to an object called fit2\nsummary(fit2) ##generate information about linear regression equation with p-values\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = year2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nRachel Robertson contributed to this portion of Taylor’s portfolio.\n\nlibrary(dplyr)\nlibrary(skimr)\n\n\nAttaching package: 'skimr'\n\n\nThe following object is masked from 'package:naniar':\n\n    n_complete\n\nlibrary(tidyr) ##I begin by opening the additional libraries that I will need for this analysis"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#exploratory-data-analysis",
    "href": "coding-exercise/coding-exercise.html#exploratory-data-analysis",
    "title": "R Coding Exercise",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nI used the dslabs reference manual on CRAN to identify a dataset that intersted me. I chose the dataset us_contagious_diseases, which contains the variables disease (factor), state (factor), year(num), weeks_reporting(num), count(num), and population(num).I will begin by exploring this dataset with the dim(), str(), and summary() functions. The libraries needed for this analysis include tidyverse, dslabs, and ggplot2, and have been opened by running the the first code chunk.\n\ndim(us_contagious_diseases)\n\n[1] 16065     6\n\nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214       \n\n\nI found that there are 6 columns and 16065 rows in this dataset. These disease factors included are: Hepatitis A, Measles, Mumps, Pertussis, Polio, Rubella, and Smallpox.\nFor the purposes of this project, I will analyze one disease, Measels. This is because, although the U.S. is trying to erradicate the virus, vaccine undercoverage in recent years has lead to sporatic outbreaks in the U.S. in naive pockets.\nstr() reveals that there are 51 states listed, so I am curious what the 51st state is. To see all of the states listed I use the function, levels().\n\nlevels(us_contagious_diseases$state) ##Examine the levels of the factor called states\n\n [1] \"Alabama\"              \"Alaska\"               \"Arizona\"             \n [4] \"Arkansas\"             \"California\"           \"Colorado\"            \n [7] \"Connecticut\"          \"Delaware\"             \"District Of Columbia\"\n[10] \"Florida\"              \"Georgia\"              \"Hawaii\"              \n[13] \"Idaho\"                \"Illinois\"             \"Indiana\"             \n[16] \"Iowa\"                 \"Kansas\"               \"Kentucky\"            \n[19] \"Louisiana\"            \"Maine\"                \"Maryland\"            \n[22] \"Massachusetts\"        \"Michigan\"             \"Minnesota\"           \n[25] \"Mississippi\"          \"Missouri\"             \"Montana\"             \n[28] \"Nebraska\"             \"Nevada\"               \"New Hampshire\"       \n[31] \"New Jersey\"           \"New Mexico\"           \"New York\"            \n[34] \"North Carolina\"       \"North Dakota\"         \"Ohio\"                \n[37] \"Oklahoma\"             \"Oregon\"               \"Pennsylvania\"        \n[40] \"Rhode Island\"         \"South Carolina\"       \"South Dakota\"        \n[43] \"Tennessee\"            \"Texas\"                \"Utah\"                \n[46] \"Vermont\"              \"Virginia\"             \"Washington\"          \n[49] \"West Virginia\"        \"Wisconsin\"            \"Wyoming\"             \n\n\nI found that the 51st state listed is District of Columbia. Since I am unsure if the Maryland values include or exclude District of Columbia, I will leave this state in the analysis but note this in the final figure.\nsummary() reveals that the years range from 1928 - 2011 and weeks reporting accounts for missing weeks. weeks reporting ranges from 0 - 52 which indicates some years with no data. I would like to find which years have missing data. A case count of 0 for any of the diseases is not necessarily missing data unless the there were 0 weeks_reporting for that year.\nBefore cleaning I will use gg_miss_var() to explore if there are any additional missing variables.\n\ngg_miss_var(us_contagious_diseases) ##find missing values\n\n\n\n\n\n\n\n\nIt seems that some population data might be missing as well so I will also have to filter the population data that has “NA” or missing data."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#cleaning-the-data",
    "href": "coding-exercise/coding-exercise.html#cleaning-the-data",
    "title": "R Coding Exercise",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nFirst, I will remove the rows with missing population data. I will do this by using the filter function.\n\nus_contagious_diseases2 &lt;- us_contagious_diseases %&gt;% ##create a new data frame\n  drop_na(population) ##drop NA from the population factor\ngg_miss_var(us_contagious_diseases2) ##check that there are no longer missing variables\n\n\n\n\n\n\n\n\nNow that the NA values for population have been dropped, we may continue to clean the data.\nI will now remove the years that are missing data for weeks_reporting.I will find the identify the years that have a 0 value for weeks_reporting by using the filter() function and drop_na() function. I will then check the number of data rows using the skim() function.\n\nus_contagious_diseases3 &lt;- us_contagious_diseases2 %&gt;% ## creating new data frame\n  dplyr::filter(weeks_reporting != 0) %&gt;% ##finding the weeks_reporting 0 values with dplyr and setting then to NA\n  tidyr::drop_na(weeks_reporting) ##dropping the NA in weeks_reporting and dropping them\nskimr::skim(us_contagious_diseases3) ##checking the number of rows\n\n\nData summary\n\n\nName\nus_contagious_diseases3\n\n\nNumber of rows\n14228\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\ndisease\n0\n1\nFALSE\n7\nMea: 3319, Per: 2709, Hep: 2327, Pol: 1844\n\n\nstate\n0\n1\nFALSE\n51\nCal: 312, Tex: 311, Ill: 306, Mic: 306\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n1969.97\n23.07\n1928\n1948\n1973\n1989\n2011\n▆▅▆▇▅\n\n\nweeks_reporting\n0\n1\n42.11\n12.16\n1\n38\n47\n51\n52\n▁▁▁▂▇\n\n\ncount\n0\n1\n1682.64\n6226.76\n0\n17\n110\n677\n132342\n▇▁▁▁▁\n\n\npopulation\n0\n1\n4254904.47\n4792670.73\n86853\n1089254\n2827642\n5162987\n37607525\n▇▁▁▁▁\n\n\n\n\n\nThe data is now clean enough for additional processing."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-the-data",
    "href": "coding-exercise/coding-exercise.html#processing-the-data",
    "title": "R Coding Exercise",
    "section": "Processing the Data",
    "text": "Processing the Data\nI will find a flat incidence rate by diving count by population for each state.I will use the mutate function to create a new column called incidence.\n\nus_contagious_diseases4 &lt;- us_contagious_diseases3 %&gt;% \n  mutate(incidence = count/population) ##create variable called incidence based on count/population\nsummary(us_contagious_diseases4) ##confirm variable\n\n        disease               state            year      weeks_reporting\n Hepatitis A:2327   California   :  312   Min.   :1928   Min.   : 1.00  \n Measles    :3319   Texas        :  311   1st Qu.:1948   1st Qu.:38.00  \n Mumps      :1576   Illinois     :  306   Median :1973   Median :47.00  \n Pertussis  :2709   Michigan     :  306   Mean   :1970   Mean   :42.11  \n Polio      :1844   Florida      :  305   3rd Qu.:1989   3rd Qu.:51.00  \n Rubella    :1374   Massachusetts:  302   Max.   :2011   Max.   :52.00  \n Smallpox   :1079   (Other)      :12386                                 \n     count          population         incidence        \n Min.   :     0   Min.   :   86853   Min.   :0.000e+00  \n 1st Qu.:    17   1st Qu.: 1089254   1st Qu.:7.152e-06  \n Median :   110   Median : 2827642   Median :3.905e-05  \n Mean   :  1683   Mean   : 4254904   Mean   :5.678e-04  \n 3rd Qu.:   677   3rd Qu.: 5162987   3rd Qu.:2.538e-04  \n Max.   :132342   Max.   :37607525   Max.   :2.964e-02  \n                                                        \n\n\nI am going to change raw incidence rate into incidence per 100,000, which is a standard measure and allows for easier visualization of the incidence rate.\n\nus_contagious_diseases5 &lt;- us_contagious_diseases4 %&gt;% \n  mutate(incidenceper100000 = incidence * 100000) ##create new variable incidenceper100000 based on incidence times 100000\nsummary(us_contagious_diseases4) ##confirm variable\n\n        disease               state            year      weeks_reporting\n Hepatitis A:2327   California   :  312   Min.   :1928   Min.   : 1.00  \n Measles    :3319   Texas        :  311   1st Qu.:1948   1st Qu.:38.00  \n Mumps      :1576   Illinois     :  306   Median :1973   Median :47.00  \n Pertussis  :2709   Michigan     :  306   Mean   :1970   Mean   :42.11  \n Polio      :1844   Florida      :  305   3rd Qu.:1989   3rd Qu.:51.00  \n Rubella    :1374   Massachusetts:  302   Max.   :2011   Max.   :52.00  \n Smallpox   :1079   (Other)      :12386                                 \n     count          population         incidence        \n Min.   :     0   Min.   :   86853   Min.   :0.000e+00  \n 1st Qu.:    17   1st Qu.: 1089254   1st Qu.:7.152e-06  \n Median :   110   Median : 2827642   Median :3.905e-05  \n Mean   :  1683   Mean   : 4254904   Mean   :5.678e-04  \n 3rd Qu.:   677   3rd Qu.: 5162987   3rd Qu.:2.538e-04  \n Max.   :132342   Max.   :37607525   Max.   :2.964e-02  \n                                                        \n\n\nI will add a weight to this value to account for missing weeks reported. To do this I will multiply each incidence rate by (weeks_reporting/52), where weeks_reporting is the total number of weeks where cases are counted and 52 is the total number of weeks in a year.I will call this new variable IRweighted.\n\nus_contagious_diseases6 &lt;- us_contagious_diseases5 %&gt;% \n  mutate(IRweighted = incidenceper100000*(weeks_reporting/52)) ##add weight to incidence rate per 100000 to account for differing numbers of weeks that case counts are captured\nsummary(us_contagious_diseases6) ##confirm variable\n\n        disease               state            year      weeks_reporting\n Hepatitis A:2327   California   :  312   Min.   :1928   Min.   : 1.00  \n Measles    :3319   Texas        :  311   1st Qu.:1948   1st Qu.:38.00  \n Mumps      :1576   Illinois     :  306   Median :1973   Median :47.00  \n Pertussis  :2709   Michigan     :  306   Mean   :1970   Mean   :42.11  \n Polio      :1844   Florida      :  305   3rd Qu.:1989   3rd Qu.:51.00  \n Rubella    :1374   Massachusetts:  302   Max.   :2011   Max.   :52.00  \n Smallpox   :1079   (Other)      :12386                                 \n     count          population         incidence         incidenceper100000 \n Min.   :     0   Min.   :   86853   Min.   :0.000e+00   Min.   :   0.0000  \n 1st Qu.:    17   1st Qu.: 1089254   1st Qu.:7.152e-06   1st Qu.:   0.7152  \n Median :   110   Median : 2827642   Median :3.905e-05   Median :   3.9055  \n Mean   :  1683   Mean   : 4254904   Mean   :5.678e-04   Mean   :  56.7780  \n 3rd Qu.:   677   3rd Qu.: 5162987   3rd Qu.:2.538e-04   3rd Qu.:  25.3760  \n Max.   :132342   Max.   :37607525   Max.   :2.964e-02   Max.   :2964.4269  \n                                                                            \n   IRweighted       \n Min.   :   0.0000  \n 1st Qu.:   0.5211  \n Median :   3.1708  \n Mean   :  53.0719  \n 3rd Qu.:  22.7544  \n Max.   :2518.9571  \n                    \n\n\nNow that I have the weighted measure of incidence rate (per 100000), I will make some exploratory figures to examine changes in Measels incidence over time for each state."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#exploratory-figures",
    "href": "coding-exercise/coding-exercise.html#exploratory-figures",
    "title": "R Coding Exercise",
    "section": "Exploratory Figures",
    "text": "Exploratory Figures\nI will plot weighted incidence rate over time in a scatter plot and add color by state to examine potential state differences.I will make a separate figure for each disease. I had to ask ChatGPT how to plot a certain category of data from a column without making a new dataframe. this is where I found the grep1(value, dataframe$var) function and structure.\n\nMeasles_plot &lt;- ggplot(data = us_contagious_diseases6[grepl('Measles', us_contagious_diseases6$disease), ], aes(x = year, y = IRweighted, color = state)) +\n  geom_line() +\n  labs(x = 'Year', y = 'IRweighted', title = 'Measles Incidence Rates (per 100000) in the U.S. from 1928 to 2011') ##add line plot of measles in the U.S. from 1928 to 2011, depending the state\nprint(Measles_plot) ##display the line plot\n\n\n\n\n\n\n\n\nI had to open this figure to view the lines. It seems that every several years, there is a measels outbreak that hits many states in the U.S. at once witha rapid peaking incidence and then rapidly declines as the population gains immunity. The peaks are particularly high for the state in the pink/ purple shade. Between 1980 to 2000 the measels IRweighted seems to diminish to near 0.\nIt is difficult to distinguish between states, so I change the data to show an IRweighted total for all of the states. ChatGPT said that the aggregate function can be used to sum the IRweighted for the state values that are equal.\n\nagg_data_measles &lt;- aggregate(IRweighted ~ year + state, data = subset(us_contagious_diseases6, disease == 'Measles'), sum) ##This aggregate function and structure was found by using ChatGPT. The subset function was used to aggregate only the measels disease data.\n\nUSmeasles_plot &lt;- ggplot(data = agg_data_measles, aes(x = year, y = IRweighted)) +\n   geom_line() +\n   labs(x = 'Year', y = 'Total IRweighted (Measles)', title = 'Total Measels Incidence Rate (per 100000) in the U.S. from 1928 to 2011') ##Another line plot is made with ggplot, but from the aggregated data frame\n\nprint(USmeasles_plot) ##display the cumulative plot\n\n\n\n\n\n\n\n\nA seasonal epi curve can be observed for the national sum of incidence rate of measels in the U.S. There are yearly peaks from 1928 until they begin to rapidly decrease around 1965, eventually reaching close to 0 in 1980. This might be do to the MMR development and distribution in the US in 1971.\nMMR elicits protection against Measels, Mumps, and Rubella. Because of the vaccine distribution, we should also see a decrease in Mumps incidence. To determine whether this is the case, I will also plot the Mumps incidence rate over time in comparison to the latest plot.\n\nagg_data_mumps &lt;- aggregate(IRweighted ~ year + state, data = subset(us_contagious_diseases6, disease == 'Mumps'), sum) ##Making a new aggregated data frame from the Mumps disease data\n\n## Plot the aggregated data for Mumps instead of measels\nUSmumps_plot &lt;- ggplot(data = agg_data_mumps, aes(x = year, y = IRweighted)) +\n   geom_line() +\n   labs(x = 'Year', y = 'Total IRweighted (Mumps)', title = 'Total Mumps Incidence Rate (per 100000) in the U.S. from 1928 to 2011')\n\nprint(USmumps_plot) ##Display the Mumps IR plot\n\n\n\n\n\n\n\n\nThough with much lower incidence rates, the mumps data set also shows a rapid decrease in incidence rates from 1970 to 1980. There is no incidence rate data prior to around 1965. For this reason, I will also plot Rubella.\n\nagg_data_rubella &lt;- aggregate(IRweighted ~ year + state, data = subset(us_contagious_diseases6, disease == 'Rubella'), sum) ##Making a new aggregated data frame from the Mumps disease data\n\n## Plot the aggregated data for Mumps instead of measels\nUSrubella_plot &lt;- ggplot(data = agg_data_rubella, aes(x = year, y = IRweighted)) +\n   geom_line() +\n   labs(x = 'Year', y = 'Total IRweighted (Rubella)', title = 'Total Rubella Incidence Rate (per 100000) in the U.S. from 1928 to 2011')\n\nprint(USrubella_plot) ## Display the rubella IR plot\n\n\n\n\n\n\n\n\nThe Rubella data shows a similar decrease from 1970 to 1980 but does not include data before around 1965. This makes it unclear whether the decrease in all three disease incidence rates is due to the MMR vaccine."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#simple-statistical-models",
    "href": "coding-exercise/coding-exercise.html#simple-statistical-models",
    "title": "R Coding Exercise",
    "section": "Simple Statistical Models",
    "text": "Simple Statistical Models\nI am interested in looking at the MMR incidence rates decrease over time. For this, I can use the total IRweighted aggregate data as the outcome and compare it to the year as the predictor.\nI will start with Measels\n\nMeasels_fit &lt;- lm(IRweighted~ year, data = agg_data_measles) ##create a linear model with Measels as outcome and year as predictor and save it to an object called Measels_fit\nMeasels_fit_table &lt;- broom::tidy(Measels_fit) ##Adding a simplified table of the linear model with tidy\nprint(Measels_fit_table) ##print table of the linear regression equation with p-values\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept) 13793.     400.         34.4 1.66e-222\n2 year           -6.94     0.204     -34.0 1.32e-217\n\n\nMeasels incidence rate (per 100000) decreased by 6.9 units per year in the U.S. This is a significant decrease with a p-value that is alpha&lt;0.05.\nI will now use the lm() function to produce a linear model Mumps over year\n\nMumps_fit &lt;- lm(IRweighted~ year, data = agg_data_mumps) ##create a linear model with Mumps as outcome and year as predictor and save it to an object called Mumps_fit\nMumps_fit_table &lt;- broom::tidy(Mumps_fit) ##Adding a simplified table of the linear model with tidy\nprint(Mumps_fit_table) ##print table of the linear regression equation with p-values\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  3596.    164.          21.9 3.91e-93\n2 year           -1.80    0.0827     -21.8 1.77e-92\n\n\nMumps incidence rates (per 100000) decreased by 1.8 per one year. This is a smaller unit change, but still a significant decrease with a p-value of alpha&lt;0.005.\nLastly, I will confirm this relationship with the Rubella aggregate data in the U.S.\n\nRubella_fit &lt;- lm(IRweighted~ year, data = agg_data_rubella) ##create a linear model with rubella as outcome and year as predictor and save it to an object called Rubella_fit\nRubella_fit_table &lt;- broom::tidy(Rubella_fit) ##Adding a simplified table of the linear model with tidy\nprint(Rubella_fit_table) ##print table of the linear regression equation with p-values\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept) 1646.      83.6         19.7 3.49e-76\n2 year          -0.827    0.0422     -19.6 1.46e-75\n\n\nThere is decrease in Rubella incidence rates (per 100000) of 0.83 over one year. This decrease, although the smallest, is significant with a p-value of alpha&lt;0.05.\nOverall, these exploratory analysis, figures, and models displayed a significant decrease in Measels, Mumps, and Rubella incident rates in the U.S. following the introduction of the MMR vaccine in 1971."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 General Background Information",
    "text": "3.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Description of data and data source",
    "text": "3.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Questions/Hypotheses to be addressed",
    "text": "3.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Data aquisition",
    "text": "4.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Data import and cleaning",
    "text": "4.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Statistical analysis",
    "text": "4.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Exploratory/Descriptive analysis",
    "text": "5.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nRace\n0\n1\n1\n4\n0\n4\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nAge\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n42.77778\n20.18525\n16\n23\n45\n51\n77\n▆▁▇▂▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Basic statistical analysis",
    "text": "5.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\nFigure 2 shows a boxplot figure produced by one of the R scripts. Each boxplot represents the distribution of height for a specific race. The average height is highest among Black participants, and average height seems to be approximately the same across the other three races included, which are White, American Indian and Alaska Native, and Asian. The boxplot for American Indian and Alaska Native is not helpful because there is only one observation for this racial category, so there is no additional data to display besides the height of the one AIAN observation.\n\n\n\n\n\n\n\n\nFigure 2: Height statified by race.\n\n\n\n\n\nFigure 3 shows a scatterplot figure produced by one of the R scripts. The scatterplot has been fitted with a line that displays the relationship between the two numerical variables along with a shaded area that represents the confidence interval around the estimates. As weight increases across the x-axis, age decreases. This observation is counter intuitive because most people gain weight as they age, which suggests there could be an issue with the data.\n\n\n\n\n\n\n\n\nFigure 3: Plot of age by weight."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Full analysis",
    "text": "5.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\nExample Table 3 shows a summary of another linear model fit with Height as the outcome with Race and Age as predictors. The table displays an estimate for each component of the linear model, along with the standard error, test statistic, and p-value for the estimate to provide context for its usefulness. The p-values for each variable are extremely large, which shows we fail to reject the null hypothesis that each of these variables are not statistically significantly associated with height according to this dataset.\n\n\n\n\nTable 3: Second linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n167.5410539\n20.2483086\n8.2743234\n0.0011643\n\n\nRaceAIAN\n-0.5460615\n26.1275182\n-0.0208999\n0.9843265\n\n\nRaceB\n-1.1524256\n18.6903702\n-0.0616588\n0.9537925\n\n\nRaceW\n-2.0799817\n20.6436086\n-0.1007567\n0.9245919\n\n\nAge\n-0.0226135\n0.4010505\n-0.0563856\n0.9577388"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 Summary and Interpretation",
    "text": "6.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.2 Strengths and Limitations",
    "text": "6.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.3 Conclusions",
    "text": "6.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "",
    "text": "Brian McKay1, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\nMark Ebell, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\nAriella Perry, Colorado Department of Public Health and Environment, Denver, CO, USA\nYe Shen, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\nAndreas Handel1, Department of Epidemiology and Biostatistics, The University of Georgia, Athens, GA, USA\n\n1 Corresponding Authors: Brian McKay and Andreas Handel\nAddress: 101 Buck Rd, Miller Hall, Athens, Georgia 30606\nEmail: bmckay52@uga.edu or ahandel@uga.edu\nPROCEEDINGS OF THE ROYAL SOCIETY B\nDOI: 10.1098/rspb.2020.0496"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#getting-the-files",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#getting-the-files",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Getting the files",
    "text": "Getting the files\nThe files described below are available from Dryad https://doi.org/10.5061/dryad.51c59zw4v.\n\n“Virulence_Trade-off.Rproj” This file lets R know the relative file paths for loading and saving files.\n“SymptomActivity.bib” This file has all of the citation saved as a bibTex.\n“Symptom Questionnaire_Redacted.pdf”: This is a copy of the electronic questionnaire patients with an upper respiratory symptoms were required to fill out. All identifying information has been redacted.\n“DataDictionary_VirulenceTradeOff.xlsx”: This document provides a description of all the variables included in the analysis.\n“1 Anonymized Data” folder contains the de-identified data for the analyses\n\nAn R script that merges all of the individual data sets and creates and saves “Data.Rda” and “Data.csv” in the “1 Anonymized Data” Folder. This script is not included since the raw data sets are not included to protect patient privacy.\n\n“2 Data Cleaning Script” folder contains one R script that cleans the data for the analyses\n\n“Data Merging Script.R”: R script that merges all of the individual data sets and creates and saves “Data.Rda” in the “1 Anonymized Data” Folder. This script is not included since the raw data sets are not included to protect patient privacy.\n“Data Cleaning.R”: This R script does all of the data preparation, creating all the required variables for the analysis. This script also produces the data sets used for the analyses and saves them in the “3 Clean Data” folder.\n\n“4 Analysis Scripts” folder has 4 R scripts that analyze the clean data and produce the results presented in the main text and supplement.\n\n“Flu Symptoms Activity Models.R” This script creates the univariate/multivariate linear regression table, the Spearman rank correlation, and the CMH trend tests. Results are saved in “5 Results”\n“Flu Symptoms Activity Plots.R” This script creates all of the plots. Results are saved in “5 Results”\n“Flu Symptoms Activity Tables.R” This script creates all of the tables. Results are saved in “5 Results”\n“Multivariate Subset Selection.R” This script does the variable selection for the multivariate model. Results are saved in “5 Results”\n\n“6 Manuscript” folder has 2 files in it used to create the manuscript.\n\n“Manuscript.Rmd” This Rmd file creates the basic manuscript word document (formatting will not be identical)\n“proceedings-of-the-royal-society-b.csl” is a style file to format the citations in the manuscript\n\n“7 Supplemental Material” folder has 2 files used to create this document\n\n“Supplemental Material.Rmd” This Rmd file creates the basic supplemental material word document (formatting will not be identical)\n“proceedings-of-the-royal-society-b.csl” is a style file to format the citations in the supplement"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#reproducing-results",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#reproducing-results",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Reproducing results",
    "text": "Reproducing results\nThe files required to reproduce the results are 2 R Markdown files, 4 R script, and one anonymized data file. These files allow the reproduction of all results shown in the main text and SM. To reproduce the results follow these steps.\nFirst, install R, Rstudio, and Pandoc (when you install Rstudio Pandoc should automatically install). Microsoft Word or Open Office Word is also required.\nSecond, save the zip file from Dryad on your local computer. Open the folder and double click “Virulence_Trade-off”. This should open Rstudio (if prompted, select Rstudio as the app to open this file type). Then open and run the files below in the specified order.\n\nR script “Data Cleaning.R” in the “2 Data Cleaning Script” folder uses “Data.Rda” and produces two clean data sets used for all further analyses. The data sets are all saved in the “3 Clean Data” folder and include:\n\n“SympAct_Any_Pos.Rda” Contains data for all influenza patients regardless of diagnosis method.\n“SympAct_Lab_Pos.Rda” Contains data for influenza patients diagnosed based on a PCR or rapid antigen test.\n\n\nIt is important to note that “SympAct_Lab_Pos.Rda” is a subset of “SympAct_Any_Pos.Rda” based on the method of diagnosis.\n\nFour R scripts in the “4 Analysis Scripts” folder (“Flu Symptoms Activity Univariate Model.R”, “Flu Symptoms Activity Univariate Plots.R”, and “Flu Symptoms Activity Univariate Tables.R”, “Multivariate Subset Selection.R”). The order you run these scripts does not matter. Results of each script are automatically saved in the “5 Results” folder.\nR Markdown “Manuscript.Rmd” is in the “6 Manuscript” folder. This combines all the relevant results and creates the main text as a Word document (some reformatting is required).\nR Markdown “Supplemental Material.Rmd” in the “7 Supplemental Material” folder generates the supplementary material as Word document."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#histogram-of-reported-activity-levels",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#histogram-of-reported-activity-levels",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Histogram of reported activity levels",
    "text": "Histogram of reported activity levels\nReported activity levels ranging from 0 to 10 with a median of 4 for those patients with a lab diagnosis of influenza (SM Figure @ref(fig:ActivityLabBarChart))\n\n\n\n\n\nHistogram of reported activity levels for patients with a lab diagnosis of influenza."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#table-of-symptoms",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#table-of-symptoms",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Table of symptoms",
    "text": "Table of symptoms\nSM Table @ref(tab:SympLabTable) shows the symptoms among patients with a lab-based diagnosis.\n\n\n\nSymptoms of the 324 patients with laboratory based flu diagnosis. The table shows the number of patients who reported having the following symptoms and the corresponding percentage.\n\n\n\nOverall\n\n\n\n\nn\n324\n\n\nAbdominal Pain = Yes (%)\n38 (11.7)\n\n\nBreathlessness = Yes (%)\n128 (39.5)\n\n\nChest Congestion = Yes (%)\n194 (59.9)\n\n\nChest Pain = Yes (%)\n110 (34.0)\n\n\nChills/Sweats = Yes (%)\n287 (88.6)\n\n\nCough = Yes (%)\n306 (94.4)\n\n\nDiarrhea = Yes (%)\n40 (12.3)\n\n\nEar Pain = Yes (%)\n59 (18.2)\n\n\nEye Pain = Yes (%)\n48 (14.8)\n\n\nFatigue = Yes (%)\n304 (93.8)\n\n\nHeadache = Yes (%)\n273 (84.3)\n\n\nItchy Eyes = Yes (%)\n75 (23.1)\n\n\nMyalgia = Yes (%)\n290 (89.5)\n\n\nNasal Congestion = Yes (%)\n255 (78.7)\n\n\nNausea = Yes (%)\n119 (36.7)\n\n\nRunny Nose = Yes (%)\n234 (72.2)\n\n\nSleeplessness = Yes (%)\n183 (56.5)\n\n\nSneeze = Yes (%)\n177 (54.6)\n\n\nSore Throat = Yes (%)\n265 (81.8)\n\n\nSubjective Fever = Yes (%)\n242 (74.7)\n\n\nSwollen Lymph Nodes = Yes (%)\n127 (39.2)\n\n\nTooth Pain = Yes (%)\n60 (18.5)\n\n\nVomiting = Yes (%)\n43 (13.3)\n\n\nWeakness = Yes (%)\n306 (94.4)\n\n\nWheezing = Yes (%)\n105 (32.4)"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#univariate-and-subset-selection",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#univariate-and-subset-selection",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Univariate and subset selection",
    "text": "Univariate and subset selection\nCorrelations between activity level and each symptom from the univariate linear analysis and the multivariate regression model selected using cross-validation (SM Table @ref(tab:LmActvSympLab)).\n\nResults of the univariate and multivariate linear regression of symptoms and activity. The coefficients are the estimated effect on activity when the symptom is present. The multivariate model was selected with a sequential forward floating selection, minimizing the RMSE on test data through a 5-fold cross validation (20 times repeated). 95%CI = The 95% confidence interval for the coefficient.\n\n\n\n\n\n\n\n\n\nDependent: Activity Level\n\nMean (sd)\nCoefficient (univariable)\nCoefficient (multivariable)\n\n\n\n\nAbdominal Pain\nNo\n4.4 (2.6)\n-\n-\n\n\n\nYes\n3.4 (2.7)\n-1.01 (-1.90 to -0.12, p=0.026)\n-\n\n\nBreathlessness\nNo\n4.3 (2.7)\n-\n-\n\n\n\nYes\n4.1 (2.5)\n-0.21 (-0.81 to 0.38, p=0.477)\n-\n\n\nChest Congestion\nNo\n4.7 (2.9)\n-\n-\n\n\n\nYes\n4.0 (2.4)\n-0.65 (-1.24 to -0.07, p=0.029)\n-\n\n\nChest Pain\nNo\n4.4 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.6)\n-0.38 (-0.99 to 0.23, p=0.217)\n-\n\n\nChills/Sweats\nNo\n5.8 (2.8)\n-\n-\n\n\n\nYes\n4.1 (2.6)\n-1.75 (-2.64 to -0.86, p&lt;0.001)\n-0.90 (-1.79 to -0.02, p=0.046)\n\n\nCough\nNo\n4.2 (3.2)\n-\n-\n\n\n\nYes\n4.3 (2.6)\n0.10 (-1.16 to 1.36, p=0.875)\n-\n\n\nDiarrhea\nNo\n4.4 (2.6)\n-\n-\n\n\n\nYes\n3.6 (2.6)\n-0.73 (-1.60 to 0.15, p=0.103)\n-\n\n\nEar Pain\nNo\n4.4 (2.6)\n-\n-\n\n\n\nYes\n3.7 (2.7)\n-0.69 (-1.44 to 0.05, p=0.068)\n-\n\n\nEye Pain\nNo\n4.2 (2.6)\n-\n-\n\n\n\nYes\n4.4 (2.9)\n0.21 (-0.61 to 1.02, p=0.619)\n-\n\n\nFatigue\nNo\n5.7 (2.5)\n-\n-\n\n\n\nYes\n4.2 (2.6)\n-1.53 (-2.72 to -0.34, p=0.012)\n-\n\n\nHeadache\nNo\n5.6 (2.8)\n-\n-\n\n\n\nYes\n4.0 (2.6)\n-1.57 (-2.35 to -0.80, p&lt;0.001)\n-1.23 (-1.97 to -0.49, p=0.001)\n\n\nSleeplessness\nNo\n4.9 (2.7)\n-\n-\n\n\n\nYes\n3.8 (2.5)\n-1.14 (-1.71 to -0.57, p&lt;0.001)\n-0.91 (-1.44 to -0.37, p=0.001)\n\n\nItchy Eyes\nNo\n4.4 (2.7)\n-\n-\n\n\n\nYes\n3.7 (2.5)\n-0.72 (-1.40 to -0.04, p=0.038)\n-\n\n\nMyalgia\nNo\n5.4 (2.9)\n-\n-\n\n\n\nYes\n4.1 (2.6)\n-1.32 (-2.25 to -0.38, p=0.006)\n-\n\n\nNasal Congestion\nNo\n4.4 (2.7)\n-\n-\n\n\n\nYes\n4.2 (2.6)\n-0.22 (-0.93 to 0.49, p=0.542)\n-\n\n\nNausea\nNo\n4.7 (2.7)\n-\n-\n\n\n\nYes\n3.6 (2.4)\n-1.08 (-1.67 to -0.49, p&lt;0.001)\n-\n\n\nSore Throat\nNo\n4.4 (2.6)\n-\n-\n\n\n\nYes\n4.2 (2.7)\n-0.20 (-0.95 to 0.55, p=0.605)\n-\n\n\nRunny Nose\nNo\n4.6 (2.6)\n-\n-\n\n\n\nYes\n4.1 (2.7)\n-0.50 (-1.14 to 0.15, p=0.129)\n-\n\n\nSneeze\nNo\n4.6 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.6)\n-0.67 (-1.24 to -0.09, p=0.024)\n-\n\n\nSubjective Fever\nNo\n5.2 (2.5)\n-\n-\n\n\n\nYes\n3.9 (2.6)\n-1.25 (-1.90 to -0.60, p&lt;0.001)\n-0.67 (-1.32 to -0.01, p=0.047)\n\n\nSwollen Lymph Nodes\nNo\n4.4 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.5)\n-0.46 (-1.05 to 0.13, p=0.128)\n-\n\n\nTooth Pain\nNo\n4.3 (2.6)\n-\n-\n\n\n\nYes\n4.1 (2.7)\n-0.24 (-0.98 to 0.50, p=0.526)\n-\n\n\nVomiting\nNo\n4.5 (2.6)\n-\n-\n\n\n\nYes\n2.8 (2.2)\n-1.64 (-2.48 to -0.81, p&lt;0.001)\n-1.39 (-2.18 to -0.60, p=0.001)\n\n\nWeakness\nNo\n6.6 (2.4)\n-\n-\n\n\n\nYes\n4.1 (2.6)\n-2.49 (-3.72 to -1.25, p&lt;0.001)\n-1.50 (-2.69 to -0.31, p=0.014)\n\n\nWheezing\nNo\n4.4 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.5)\n-0.46 (-1.07 to 0.16, p=0.144)\n-"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#distribution-of-scores",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#distribution-of-scores",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Distribution of scores",
    "text": "Distribution of scores\nThe infectiousness score is skewed left with more than half of the patients having a score of 4 or 5 (SM Figure @ref(fig:IandMLabBarChart)A). The morbidity score is more centered with no patient having a score of 0,1,19 or 20 (SM Figure @ref(fig:IandMLabBarChart)B).\n\n\n\n\n\n(A) Histogram of infectiousness score for patients with a lab diagnosis of influenza. (B) Histogram of morbidity score for patients with a lab diagnosis of influenza."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#correlation-between-number-and-severity-of-symptoms",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#correlation-between-number-and-severity-of-symptoms",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Correlation between number and severity of symptoms",
    "text": "Correlation between number and severity of symptoms\nThe data we have available reports most of the symptoms as absence or presence. If more symptoms correlate positively with stronger symptoms, using absence/presence data only is a valid, albeit less powerful approach. More problematically is a situation in which the number and strength of symptoms correlate negatively. For example, a person having fewer symptoms might “make up” for the lower number of symptoms by having a more severe form. Such a relation would invalidate our approach of adding presence/absence symptoms to arrive at a given score. While we cannot thoroughly test potential correlations between the quantity and strength of symptoms on all our data, we do have severity information on the cough, weakness, and body aches symptoms. This allows us to evaluate the relation between the number of symptoms and the severity of those symptoms.\nWe find that as the number of infectiousness-related symptoms (i.e., our infectiousness scores) increases, there is an increase in cough intensity (SM Figure @ref(fig:Infect1CoughFig)). The same relationship is found between the number of morbidity-related symptoms (i.e., our morbidity scores) and the severity of weakness and body aches (SM Figures @ref(fig:Morbidity1WeaknessFig) and @ref(fig:Morbidity1BodyAchesFig)). While not conclusive, this evidence suggests that our assumption that symptom absence/presence also captures symptom severity seems to be defensible.\n\n\n\n\n\nRelationship between cough intensity and the infectiousness score from main text using all Y/N variables plausibly related to infectiousness.\n\n\n\n\n\n\n\n\n\nRelationship between weakness intensity and the morbidity score from main text using all Y/N variables related to morbidity.\n\n\n\n\n\n\n\n\n\nRelationship between body ache intensity and the morbidity score from main text using all Y/N variables related to morbidity."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#alternative-approaches-to-calculate-the-infectiousness-score",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#alternative-approaches-to-calculate-the-infectiousness-score",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Alternative approaches to calculate the infectiousness score",
    "text": "Alternative approaches to calculate the infectiousness score\nIn the main text, we use all of the available yes/no symptoms that can feasibly be related to infectiousness. Since there is no consensus about which symptoms or routes of transmission are the most important, we also consider how robust the results are using several different versions of infectiousness scores.\nWhile chest congestion might be a proxy for pathogen load, i.e., more congestion could indicate higher levels of a pathogen, in which case it could impact infectiousness. However, it is true that chest congestion alone without coughing or sneezing might not lead to increased infectiousness (though some recent studies suggest that breathing alone accounts for a large fraction of expelled influenza virions [1]). To explore if inclusion or exclusion of chest congestion in the infectiousness scores made a difference, we created a score excluding it to see if the overall conclusions would change. The new score values ranged from 0 to 4 (SM Figure @ref(fig:InfectScore2Fig)A).\nThe same overall relationship is observed between the new infectiousness score and activity. There is still a curve. Spearman’s rank correlation indicates a negative relationship with (\\(r=\\) -0.11 (95% CI: -0.22, -0.00)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 3.772, \\(df =\\) 1, \\(p\\) 0.05) (SM Figure @ref(fig:InfectScore2Fig)B).\nWhen compared to the morbidity score, the same overall trends from the primary analysis were observed. Spearman’s rank correlation (\\(r=\\) 0.17 (95% CI: 0.06, 0.27)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 11.323, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 1.5 fold increase in the new infectiousness score going from the lowest to the highest morbidity score (SM Figure @ref(fig:InfectScore2Fig)C).\n\n\n\n\n\nInfectiousness score without chest congestion\n\n\n\n\nHow patients define congestion symptoms is very subjective [2], so we created a score excluding both of the congestion-related variables to see if the overall conclusions would change. The new score could have a value of 0 to 3 (SM Figure @ref(fig:InfectScore3Fig)A).\nThe same overall relationship is observed between the new infectiousness score and activity. The curved relationship is not as clear but still present. Spearman’s rank correlation indicates negative relationship with (\\(r=\\) -0.11 (95% CI: -0.22, -0.00)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 4.599, \\(df =\\) 1, \\(p\\) 0.03) (SM Figure @ref(fig:InfectScore3Fig)B).\nWhen compared to the morbidity score again, the same trends were observed, and Spearman’s rank correlation indicates positive relationship (\\(r=\\) 0.16 (95% CI: 0.05, 0.26)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 10.023, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 1.5 fold increase in the new infectiousness score going from the lowest to the highest morbidity score (SM Figure @ref(fig:InfectScore3Fig)C).\n\n\n\n\n\nInfectiousness score without chest and nasal congestion variables\n\n\n\n\nFor the score used in the main text, we included all relevant symptoms even if they can be considered strongly related. This might lead to potential double-counting of some symptoms. To evaluate if this might cause problems, we created an alternative score that removed highly correlated variables based on two cut off values of Yule’s Q [3]. There is no commonly used value to define a cut-off at which correlated variables should be removed. We decided to use an absolute correlation of 0.9 or 0.75 for our cut off. When using the 0.9 cut off only very strongly correlated variables were removed, while for the 0.75 even intermediately strong correlations lead to the removal of one of the variables [4]. For any pair of symptoms with a positive or negative correlation of 0.9 or greater, the more informative symptom (having the proportion Yes/No closest to 50%) was kept[5]. For our data, cough and chest congestion had a Yule’s Q of 0.933, and chest congestion had the best balance between yes and no responses, so it was kept. When relaxing the correlation to 0.75, there is no change in the variables included in the score.\nThe score values range from 0 to 4 (SM Figure @ref(fig:InfectScore4Fig)A). The same overall relationship is observed between the new infectiousness score and activity. The curve is present, Spearman’s rank correlation indicates negative relationship with (\\(r=\\) -0.16 (95% CI: -0.26, -0.05)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 7.138, \\(df =\\) 1, \\(p\\) &lt; 0.01) (SM Figure @ref(fig:InfectScore4Fig)B).\nWhen compared to the morbidity score again the same overall trends were observed with Spearman’s rank correlation indicates positive relationship (\\(r=\\) 0.28 (95% CI: 0.18, 0.38)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 25.818, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 2.1 fold increase in the new infectiousness score going from the lowest to the highest morbidity score (SM Figure @ref(fig:InfectScore4Fig)C).\n\n\n\n\n\nInfectiousness score removing variables determined to be redundant based on Yule’s Q"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#alternative-approaches-to-calculate-morbidity-score",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#alternative-approaches-to-calculate-morbidity-score",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Alternative approaches to calculate morbidity score",
    "text": "Alternative approaches to calculate morbidity score\nTo see the impact of removing correlated variables from the morbidity score, we used the same methods applied to the infectiousness score above. Among the morbidity symptoms, only vomiting and weakness correlated greater than 0.9. Vomiting was included in the score since it was more balanced then weakness, which was present in 94% of patients. This score using 0.9 cut off had a possible range of 0 to 19. There are no patients with a morbidity score of 0,1,18,19 (SM Figure @ref(fig:ImpactScore2Fig)A). The same relationships observed in the main text are seen when the morbidity score is compared to activity and infectiousness scores.\nThere is a negative correlation between the morbidity score, and the patient’s self-reported activity level suggests that higher morbidity score is associated with reduced activity levels. Spearman’s rank correlation indicates negative relationship (\\(r=\\) -0.31 (95% CI: -0.41, -0.21)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 36.004, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 4.3 fold decrease in activity level going from the lowest to the highest morbidity score (SM Figure @ref(fig:ImpactScore2Fig)B).\nThere is a positive correlation between the morbidity and infectiousness scores show a positive correlation. Spearman’s rank correlation indicates positive relationship (\\(r=\\) 0.28 (95% CI: 0.18, 0.38)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 25.942, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 1.7 fold in the infectiousness score going from the lowest to the highest morbidity score (SM Figure @ref(fig:ImpactScore2Fig)C).\n\n\n\n\n\nMorbidity score removing variables determined to be redundant using the absolute value of Yule’s Q of 0.9 or more\n\n\n\n\nThe score created using the 0.75 cut off was different, with a total of 6 symptoms being excluded. Starting with the highest correlations first: Weakness/Vomit (Q=1) keep vomit, Tooth pain/Headache (Q=.86) keep Tooth pain, Chills Sweats/SoreThroat (Q=-.8) keep Chills Sweats, Fatigue/BodyAches (Q=.80) keep BodyAches, Vomit/Nausea (Q=.79) keep Nausea, SubjectiveFever/ChillsSweats (Q=.76) keep SubjectiveFever. The 0.75 cut off morbidity score included Subjective Fever, Myalgia, Sleeplessness, Breathlessness, Wheezing, Chest pain, Abdominal Pain, Diarrhea, Nausea, Ear Pain, Tooth pain, Eye pain, Itchy Eyes, and Swollen Lymph Nodes. This morbidity score has a possible value of 0 to 14. Compared to the morbidity score used in the main text and 0.9 cut off there are now 12 patients with a score of 0 or 1, but there are still no patients with a score of 13 or 14. (SM Figure @ref(fig:ImpactScore3Fig)A). Again the overall results remain the same despite using a different version of the score.\nThere is a negative correlation between the morbidity score and the patient’s self-reported activity level. Spearman’s rank correlation indicates negative relationship (\\(r=\\) -0.26 (95% CI: -0.36, -0.16)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 24.987, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 2.9 fold reduction in activity level going from the lowest to the highest morbidity score (SM Figure @ref(fig:ImpactScore3Fig)B).\nThe relationship between the morbidity and infectiousness scores show a positive correlation. Spearman’s rank correlation indicates positive relationship (\\(r=\\) 0.29 (95% CI: 0.18, 0.38)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 25.942, \\(df =\\) 1, \\(p\\) &lt; 0.01). We find a mean 1.7 fold increase in the infectiousness score going from the lowest to the highest morbidity score (SM Figure @ref(fig:ImpactScore3Fig)C).\n\n\n\n\n\nMorbidity score removing variables determined to be redundant using the absolute value of Yule’s Q of 0.75 or more"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#description-of-the-population",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#description-of-the-population",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Description of the population",
    "text": "Description of the population\nInfluenza diagnosis for our population is determined using three different methods a rapid antigen test, a PCR test, or an empirical diagnosis by a physician. In the main text, we considered any person who was diagnosed by either a rapid antigen or PCR test as having influenza. Here we repeat the analyses completed in the main text with the addition of patients with a diagnosis of influenza empirically based on symptoms. Patients with an empirical diagnosis are generally defined as having influenza-like illness (ILI). In total, there are 735 patients with any diagnosis of influenza. Among these patients, the activity levels ranged from 0 to 10, with a median of 4. All of the patients had symptoms of disease. The most common symptom is weakness, and the least common symptom is vomiting (SM Table @ref(tab:SympAnyTable)).\n\n\n\nOut of the 735 patients included the table shows the number of patients who reported having the following symptoms and the corresponding percentage.\n\n\n\nOverall\n\n\n\n\nn\n735\n\n\nAbdominal Pain = Yes (%)\n93 (12.7)\n\n\nBreathlessness = Yes (%)\n297 (40.4)\n\n\nChest Congestion = Yes (%)\n409 (55.6)\n\n\nChest Pain = Yes (%)\n234 (31.8)\n\n\nChills/Sweats = Yes (%)\n604 (82.2)\n\n\nCough = Yes (%)\n660 (89.8)\n\n\nDiarrhea = Yes (%)\n99 (13.5)\n\n\nEar Pain = Yes (%)\n162 (22.0)\n\n\nEye Pain = Yes (%)\n113 (15.4)\n\n\nFatigue = Yes (%)\n671 (91.3)\n\n\nHeadache = Yes (%)\n620 (84.4)\n\n\nItchy Eyes = Yes (%)\n182 (24.8)\n\n\nMyalgia = Yes (%)\n656 (89.3)\n\n\nNasal Congestion = Yes (%)\n565 (76.9)\n\n\nNausea = Yes (%)\n258 (35.1)\n\n\nRunny Nose = Yes (%)\n524 (71.3)\n\n\nSleeplessness = Yes (%)\n419 (57.0)\n\n\nSneeze = Yes (%)\n395 (53.7)\n\n\nSore Throat = Yes (%)\n614 (83.5)\n\n\nSubjective Fever = Yes (%)\n505 (68.7)\n\n\nSwollen Lymph Nodes = Yes (%)\n314 (42.7)\n\n\nTooth Pain = Yes (%)\n166 (22.6)\n\n\nVomiting = Yes (%)\n79 (10.7)\n\n\nWeakness = Yes (%)\n686 (93.3)\n\n\nWheezing = Yes (%)\n221 (30.1)"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#univariate-and-subset-selection-1",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#univariate-and-subset-selection-1",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Univariate and subset selection",
    "text": "Univariate and subset selection\nWe explored the univariate correlations between activity level and each symptom. All of the symptoms that were statistically significantly related to activity showed a negative correlation with activity level (SM Table @ref(tab:LmActvSympAny)). Based on the cross-validated variable selection, we found that a model that included chills/sweats, subjective fever, headache, weakness, sleeplessness, and vomiting creates the most predictive model (SM Table @ref(tab:LmActvSympAny)).\n\nResults of the univariate and multivariate linear regression of symptoms and activity. The coefficients are the estimated effect on activity when the symptom is present. The multivariate model was selected with a sequential forward floating selection, minimizing the root mean square error on test data through a 5-fold cross validation (20 times repeated). 95%CI = The 95% confidence interval for the coefficient.\n\n\n\n\n\n\n\n\n\nDependent: Activity Level\n\nMean (sd)\nCoefficient (univariable)\nCoefficient (multivariable)\n\n\n\n\nAbdominal Pain\nNo\n4.6 (2.6)\n-\n-\n\n\n\nYes\n3.8 (2.7)\n-0.78 (-1.35 to -0.20, p=0.008)\n-\n\n\nBreathlessness\nNo\n4.6 (2.7)\n-\n-\n\n\n\nYes\n4.2 (2.6)\n-0.39 (-0.78 to 0.00, p=0.052)\n-\n\n\nChest Congestion\nNo\n4.7 (2.7)\n-\n-\n\n\n\nYes\n4.2 (2.5)\n-0.49 (-0.88 to -0.11, p=0.012)\n-\n\n\nChest Pain\nNo\n4.6 (2.6)\n-\n-\n\n\n\nYes\n4.2 (2.8)\n-0.42 (-0.83 to -0.01, p=0.044)\n-\n\n\nChills/Sweats\nNo\n6.2 (2.5)\n-\n-\n\n\n\nYes\n4.1 (2.5)\n-2.08 (-2.56 to -1.61, p&lt;0.001)\n-1.30 (-1.79 to -0.81, p&lt;0.001)\n\n\nCough\nNo\n4.9 (2.8)\n-\n-\n\n\n\nYes\n4.4 (2.6)\n-0.48 (-1.11 to 0.15, p=0.137)\n-\n\n\nDiarrhea\nNo\n4.6 (2.7)\n-\n-\n\n\n\nYes\n3.7 (2.5)\n-0.85 (-1.41 to -0.29, p=0.003)\n-\n\n\nEar Pain\nNo\n4.5 (2.6)\n-\n-\n\n\n\nYes\n4.2 (2.7)\n-0.34 (-0.80 to 0.12, p=0.149)\n-\n\n\nEye Pain\nNo\n4.5 (2.7)\n-\n-\n\n\n\nYes\n4.5 (2.6)\n0.05 (-0.48 to 0.58, p=0.855)\n-\n\n\nFatigue\nNo\n5.5 (2.6)\n-\n-\n\n\n\nYes\n4.4 (2.6)\n-1.15 (-1.83 to -0.48, p=0.001)\n-\n\n\nHeadache\nNo\n5.6 (2.6)\n-\n-\n\n\n\nYes\n4.3 (2.6)\n-1.34 (-1.86 to -0.82, p&lt;0.001)\n-0.91 (-1.39 to -0.43, p&lt;0.001)\n\n\nSleeplessness\nNo\n5.0 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.5)\n-0.96 (-1.35 to -0.58, p&lt;0.001)\n-0.72 (-1.07 to -0.37, p&lt;0.001)\n\n\nItchy Eyes\nNo\n4.5 (2.7)\n-\n-\n\n\n\nYes\n4.4 (2.5)\n-0.07 (-0.52 to 0.37, p=0.742)\n-\n\n\nMyalgia\nNo\n5.5 (2.7)\n-\n-\n\n\n\nYes\n4.3 (2.6)\n-1.14 (-1.75 to -0.53, p&lt;0.001)\n-\n\n\nNasal Congestion\nNo\n4.8 (2.6)\n-\n-\n\n\n\nYes\n4.4 (2.7)\n-0.39 (-0.84 to 0.07, p=0.096)\n-\n\n\nNausea\nNo\n4.8 (2.7)\n-\n-\n\n\n\nYes\n3.8 (2.5)\n-0.99 (-1.39 to -0.60, p&lt;0.001)\n-\n\n\nSore Throat\nNo\n4.5 (2.6)\n-\n-\n\n\n\nYes\n4.5 (2.6)\n-0.02 (-0.54 to 0.50, p=0.939)\n-\n\n\nRunny Nose\nNo\n4.6 (2.7)\n-\n-\n\n\n\nYes\n4.4 (2.6)\n-0.19 (-0.61 to 0.23, p=0.382)\n-\n\n\nSneeze\nNo\n4.6 (2.7)\n-\n-\n\n\n\nYes\n4.3 (2.6)\n-0.27 (-0.66 to 0.11, p=0.164)\n-\n\n\nSubjective Fever\nNo\n5.6 (2.5)\n-\n-\n\n\n\nYes\n4.0 (2.6)\n-1.62 (-2.01 to -1.22, p&lt;0.001)\n-0.92 (-1.33 to -0.51, p&lt;0.001)\n\n\nSwollen Lymph Nodes\nNo\n4.5 (2.6)\n-\n-\n\n\n\nYes\n4.4 (2.7)\n-0.13 (-0.52 to 0.25, p=0.495)\n-\n\n\nTooth Pain\nNo\n4.6 (2.6)\n-\n-\n\n\n\nYes\n4.2 (2.7)\n-0.40 (-0.86 to 0.05, p=0.084)\n-\n\n\nVomiting\nNo\n4.6 (2.6)\n-\n-\n\n\n\nYes\n3.1 (2.3)\n-1.57 (-2.18 to -0.96, p&lt;0.001)\n-1.27 (-1.84 to -0.71, p&lt;0.001)\n\n\nWeakness\nNo\n6.3 (2.5)\n-\n-\n\n\n\nYes\n4.3 (2.6)\n-1.98 (-2.73 to -1.22, p&lt;0.001)\n-0.92 (-1.64 to -0.21, p=0.011)\n\n\nWheezing\nNo\n4.7 (2.7)\n-\n-\n\n\n\nYes\n4.0 (2.5)\n-0.67 (-1.09 to -0.26, p=0.001)\n-"
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#computation-of-infectiousness-and-morbidity-scores",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#computation-of-infectiousness-and-morbidity-scores",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Computation of infectiousness and morbidity scores",
    "text": "Computation of infectiousness and morbidity scores\nWe used the same symptom classification presented in the main text. The median infectiousness score is 4, and a skewed distribution is present with most of the patients having a score of 4 or 5 (SM Figure @ref(fig:InfectScoreFig)).\n\n\n\n\n\nDistribution of the infectiousness score.\n\n\n\n\nThe median morbidity score is 9, and no patients have a morbidity score of 0, 1, 19, 20 (SM Figure @ref(fig:MorbScoreFig)). Such a centered distribution is assumed to be a result of patients felling ill enough to seek medical care, but none were sick enough to require urgent care or hospitalization.\n\n\n\n\n\nDistribution of the morbidity score."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-infectiousness-score-on-activity",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-infectiousness-score-on-activity",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Impact of infectiousness score on activity",
    "text": "Impact of infectiousness score on activity\nAnalysis of the impact of the infectiousness score on activity suggests that the value of this score has a negative correlation with the activity level. Spearman’s rank correlation is \\(r =\\) -0.10 (95% CI: -0.17, -0.03) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 7.083, \\(df =\\) 1, \\(p =\\) &lt; 0.01) (SM Figure @ref(fig:AnyTvAFig)). This is different from the main analysis were we did not observe a clear relationship between activity and the infectiousness score.\n\n\n\n\n\nActivity level for each level of the infectiousness score. The red diamond is the mean. The solid blue line is the linear regression fit. The shaded area is the 95% confidence interval for the linear regression."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-morbidity-score-on-activity",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-morbidity-score-on-activity",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Impact of morbidity score on activity",
    "text": "Impact of morbidity score on activity\nAnalysis of the impact of the morbidity score on activity suggests that the value of this score is correlated with the activity level of a patient, with higher morbidity correlating with reduced activity. Spearman’s rank correlation indicates a negative relationship \\(r =\\) -0.32 (95% CI: -0.38, -0.25) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 78.501, \\(df =\\) 1, \\(p\\) &lt; 0.01) (SM Figure @ref(fig:AnyMvAFig)). The observed pattern is clear, with a mean 3.6 fold decrease in activity level going from the lowest to the highest morbidity score.\n\n\n\n\n\nActivity level for each level of the morbidity score. The red diamond is the mean. The solid blue line is the linear regression fit. The shaded area is the 95% confidence interval for the linear regression."
  },
  {
    "objectID": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-morbidity-score-on-infectiousness-score",
    "href": "starter-analysis-exercise/code/ProcSocB Supplemental Material/7 Supplemental Material/Supplemental Material.html#impact-of-morbidity-score-on-infectiousness-score",
    "title": "Supplemental Material: The Impact of Symptom and Activity Trade-offs on Transmission Potential of Patients Infected with Influenza",
    "section": "Impact of morbidity score on infectiousness score",
    "text": "Impact of morbidity score on infectiousness score\nAnalysis of the relationship between the morbidity and infectiousness scores show a positive correlation. Spearman’s rank correlation indicates a positive relationship ( \\(r =\\) 0.26 (95% CI: 0.19, 0.33)) and the Cochran-Mantel-Haenszel trend test is statistically significant (\\(\\chi^2 =\\) 44.505, \\(df =\\) 1, \\(p\\) &lt; 0.01) (SM Figure @ref(fig:AnyMvTFig)). Apart from the values activity levels for low morbidity score (with small sample sizes), the pattern is consistent with a mean 1.7 fold increase in the infectiousness score going from the lowest to the highest morbidity score.\n\n\n\n\n\nInfectiousness score for each level of the morbidity score. The red diamond is the mean. The solid blue line is the linear regression fit. The shaded area is the 95% confidence interval for the linear regression."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Taylor Glass Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  }
]